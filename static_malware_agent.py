import requests
import json
import subprocess
from datetime import datetime, timedelta
import os
import pandas as pd
import schedule
import time
from sklearn.ensemble import IsolationForest
from statsmodels.tsa.seasonal import STL
import numpy as np

class MalwareIntelligenceAgent:
    def __init__(self):
        self.config = {
            "sources": {
                "github_malware_db": {
                    "url": "https://github.com/aaryanrlondhe/Malware-Hash-Database.git",
                    "path": "Malware-Hash-Database",
                    "update_interval": 3600  # 1 hour
                },
                "malwarebazaar": {
                    "recent": "https://bazaar.abuse.ch/export/csv/recent/",
                    "update_interval": 900  # 15 minutes
                }
            },
            "analysis_window": timedelta(days=7),
            "alert_threshold": 0.7,
            "model": IsolationForest(contamination=0.1),
            "storage_file": "malware_data.json"
        }
        
        self.historical_data = pd.DataFrame()
        self.trend_metrics = {}
        self.init_data_storage()

    def init_data_storage(self):
        """Initialize JSON storage file"""
        if not os.path.exists(self.config['storage_file']):
            with open(self.config['storage_file'], 'w') as f:
                json.dump([], f)

    def continuous_monitoring(self):
        """Schedule data collection and analysis tasks"""
        schedule.every(self.config['sources']['github_malware_db']['update_interval']).seconds.do(
            self.update_github_repo)
        schedule.every(self.config['sources']['malwarebazaar']['update_interval']).seconds.do(
            self.fetch_malwarebazaar)
        
        schedule.every(6).hours.do(self.analyze_trends)
        
        while True:
            schedule.run_pending()
            time.sleep(1)

    def update_github_repo(self):
        """Automated repository updates"""
        try:
            if not os.path.exists(self.config['sources']['github_malware_db']['path']):
                subprocess.run(["git", "clone", self.config['sources']['github_malware_db']['url'], 
                              self.config['sources']['github_malware_db']['path']])
            else:
                subprocess.run(["git", "-C", self.config['sources']['github_malware_db']['path'], "pull"])
            self.process_github_data()
        except Exception as e:
            self.log_error(f"GitHub update failed: {str(e)}")

    def process_github_data(self):
        """Process and store GitHub repository data"""
        try:
            new_entries = []
            for root, _, files in os.walk(self.config['sources']['github_malware_db']['path']):
                for file in files:
                    if any(file.endswith(ext) for ext in ['.md5', '.sha1', '.sha256']):
                        with open(os.path.join(root, file)) as f:
                            for line in f:
                                hash_val = line.strip()
                                new_entries.append({
                                    "hash": hash_val,
                                    "type": file.split('.')[-1].upper(),
                                    "source": "GitHub-Malware-DB",
                                    "timestamp": datetime.now().isoformat()
                                })
            
            self._append_to_storage(new_entries)
        except Exception as e:
            self.log_error(f"GitHub processing failed: {str(e)}")

    def fetch_malwarebazaar(self):
        """Continuous MalwareBazaar monitoring"""
        try:
            response = requests.get(self.config['sources']['malwarebazaar']['recent'])
            df = pd.read_csv(response.text.splitlines()[7:], sep=",", quotechar='"')
            
            new_entries = []
            for _, row in df.iterrows():
                new_entries.append({
                    "hash": row['sha256_hash'],
                    "type": "SHA256",
                    "source": "MalwareBazaar",
                    "timestamp": datetime.now().isoformat(),
                    "file_type": row['file_type'],
                    "signature": row['signature']
                })
            
            self._append_to_storage(new_entries)
            self.detect_anomalies(new_entries)
        except Exception as e:
            self.log_error(f"MalwareBazaar fetch failed: {str(e)}")

    def _append_to_storage(self, new_data):
        """Append new entries to JSON file"""
        try:
            # Read existing data
            with open(self.config['storage_file'], 'r') as f:
                existing_data = json.load(f)
            
            # Merge and deduplicate
            existing_hashes = {entry['hash'] for entry in existing_data}
            unique_new = [entry for entry in new_data if entry['hash'] not in existing_hashes]
            
            # Write back combined data
            with open(self.config['storage_file'], 'w') as f:
                json.dump(existing_data + unique_new, f, indent=2)
                
        except Exception as e:
            self.log_error(f"Storage update failed: {str(e)}")

    def analyze_trends(self):
        """Periodic trend analysis"""
        try:
            with open(self.config['storage_file'], 'r') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Time window analysis
            cutoff = datetime.now() - self.config['analysis_window']
            recent_data = df[df['timestamp'] > cutoff]
            
            # Calculate trend metrics
            self.trend_metrics = {
                'hash_types': recent_data['type'].value_counts().to_dict(),
                'file_types': recent_data['file_type'].value_counts().to_dict(),
                'signatures': recent_data['signature'].value_counts().to_dict()
            }
            
            # Store trend analysis
            self._append_to_storage([{
                "type": "trend_analysis",
                "timestamp": datetime.now().isoformat(),
                "metrics": self.trend_metrics
            }])
            
        except Exception as e:
            self.log_error(f"Trend analysis failed: {str(e)}")

    def detect_anomalies(self, new_data):
        """Real-time anomaly detection"""
        try:
            df = pd.DataFrame(new_data)
            features = df.groupby(['file_type', 'signature']).size().unstack().fillna(0)
            anomalies = self.config['model'].predict(features)
            
            if np.any(anomalies == -1):
                self._append_to_storage([{
                    "type": "anomaly_alert",
                    "timestamp": datetime.now().isoformat(),
                    "data": features[anomalies == -1].to_dict()
                }])
        except Exception as e:
            self.log_error(f"Anomaly detection failed: {str(e)}")

    def log_error(self, message):
        """Centralized error logging"""
        with open('agent_errors.log', 'a') as f:
            f.write(f"{datetime.now()} - {message}\n")

    def run(self):
        """Start the intelligence agent"""
        try:
            self.continuous_monitoring()
        except KeyboardInterrupt:
            print("Shutting down intelligence agent...")

if __name__ == "__main__":
    agent = MalwareIntelligenceAgent()
    agent.run()
